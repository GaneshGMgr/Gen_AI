{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e32408",
   "metadata": {},
   "source": [
    "### Testing the Audio Generation using `parler_tts` i.e Parler Model and Bark Model\n",
    "`parler_tts` is a text-to-speech (TTS) library developed by Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163028dc",
   "metadata": {},
   "source": [
    "##### Text-To-Speech\n",
    "[ipsilondev/parler_tts](https://huggingface.co/ipsilondev/parler_tts)\n",
    "\n",
    "[ipsilondev/parler_tts](https://huggingface.co/parler-tts/parler-tts-mini-v1)\n",
    "\n",
    "uv pip install git+https://github.com/huggingface/parler-tts.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440b5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install optimum\n",
    "#!pip install -U flash-attn --no-build-isolation\n",
    "#!pip install transformers==4.43.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba5a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import IPython.display as ipd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BarkModel, AutoProcessor, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from parler_tts import ParlerTTSForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f15fdad",
   "metadata": {},
   "source": [
    "### Parler Model\n",
    "Let's try using the Parler Model first and generate a short segment with speaker Laura's voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf04a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "## Load model and tokenizer\n",
    "model = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-mini-v1\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-v1\")\n",
    "\n",
    "# Define text and description\n",
    "text_prompt = \"\"\"\n",
    "Exactly! And the distillation part is where you take a LARGE-model,and compress-it down into a smaller, more efficient model that can run on devices with limited resources.\n",
    "\"\"\"\n",
    "# text_prompt ‚Üí What to say\n",
    "# description ‚Üí How to say it (like giving instructions, mood, emotion, pacing, tone)\n",
    "description = \"\"\" \n",
    "Laura's voice is expressive and dramatic in delivery, speaking at a fast pace with a very close recording that almost has no background noise.\n",
    "\"\"\"\n",
    "\n",
    "## Tokenize inputs to convert into numerical tensors (which is what the model needs) and Sent to GPU or CPU via .to(device)\n",
    "input_ids = tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\n",
    "# input_ids: tells the model how the speaker should sound.\n",
    "# prompt_input_ids: tells the model what the speaker should say.\n",
    "prompt_input_ids = tokenizer(text_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "# Together, they allow the model to generate expressive speech in the style you describe.\n",
    "\n",
    "## Generate audio\n",
    "# model.generate(...)\tModel produces speech audio from description + text\n",
    "generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\n",
    "audio_arr = generation.cpu().numpy().squeeze() # .squeeze() removes any extra dimensions.\n",
    "\n",
    "# The model returns a PyTorch tensor (usually on GPU).\n",
    "# We move it to CPU with .cpu().\n",
    "# Then convert it to a NumPy array, which is easier to work with.\n",
    "# End result: audio_arr is a 1D array of audio samples ‚Äî just like get from a .wav file!\n",
    "\n",
    "# Play audio in notebook\n",
    "ipd.Audio(audio_arr, rate=model.config.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dad0e45",
   "metadata": {},
   "source": [
    "### üéôÔ∏è Bark Model\n",
    "\n",
    "Amazing! Let's try the same with **Bark** now:\n",
    "\n",
    "- We will set the `voice_preset` to our favorite speaker.\n",
    "- This time, we can include **expression prompts** inside our generation prompt.\n",
    "- Note:\n",
    "  - We can **CAPITALIZE** words to make the model **emphasize** them.\n",
    "  - We can use **hyphens** (`-`) to make the model **pause** on certain words.\n",
    "\n",
    "Example:\n",
    "> \"HELLO - my name is ChatGPT and I'm EXCITED - to talk with you!\"\n",
    "\n",
    "Bark interprets these cues to generate more **natural-sounding and expressive speech**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07831af",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:7\" # system has multiple GPUs, it's forcing Bark to run on the 8th GPU (index starts from 0).\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"suno/bark\") # text processing pipeline that matches the Bark model i.e\n",
    "# Tokenizing the input text, Applying the voice preset, Formatting everything for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4106b0",
   "metadata": {},
   "source": [
    "## üîß Bark TTS Model Loading Options: Speed & Optimization Guide\n",
    "\n",
    "### ‚úÖ Default Model (Standard Setup)\n",
    "```python\n",
    "model = BarkModel.from_pretrained(\"suno/bark\", torch_dtype=torch.float16).to(device)\n",
    "```\n",
    "- Uses Bark as-is, in float16 precision.\n",
    "- Compatible with all systems.\n",
    "- üê¢ Slower inference, but safe and reliable.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° Option 1: BetterTransformer\n",
    "```python\n",
    "model = BarkModel.from_pretrained(\"suno/bark\", torch_dtype=torch.float16).to(device)\n",
    "model = model.to_bettertransformer()\n",
    "```\n",
    "- Uses Hugging Face's `BetterTransformer` for optimized transformer layers.\n",
    "- ‚ö° Speeds up inference (30‚Äì50% faster).\n",
    "- ‚úÖ Uses less memory.\n",
    "- Requires **PyTorch 2.x**.\n",
    "- üß™ Simple to use ‚Äî great balance of speed & ease.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö°‚ö° Option 2: FlashAttention 2\n",
    "```python\n",
    "model = BarkModel.from_pretrained(\"suno/bark\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\n",
    "```\n",
    "- Enables **FlashAttention-2**: super fast & memory efficient attention.\n",
    "- ‚ö°‚ö° Highest performance, especially for large inputs or batching.\n",
    "- Requires:\n",
    "  - ‚úÖ PyTorch 2.x\n",
    "  - ‚úÖ CUDA >= 11.7\n",
    "  - ‚úÖ Compatible GPU (Ampere or newer: A100, 3090, 4090, etc.)\n",
    "  - ‚úÖ `flash-attn` package installed and compiled correctly\n",
    "- üîß Advanced setup ‚Äî best for power users.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Recommendation:\n",
    "| Use Case | Suggested Setup |\n",
    "|----------|-----------------|\n",
    "| ‚úÖ Just getting started | Default setup |\n",
    "| ‚öôÔ∏è Want speed with ease | `to_bettertransformer()` |\n",
    "| üß† Have modern GPU + need max speed | `flash_attention_2` |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfad25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model =  model.to_bettertransformer()\n",
    "#model = BarkModel.from_pretrained(\"suno/bark\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\n",
    "model = BarkModel.from_pretrained(\"suno/bark\", torch_dtype=torch.float16).to(device)#.to_bettertransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd9a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_preset = \"v2/en_speaker_6\" # selects a predefined voice from Bark‚Äôs library of synthetic voices\n",
    "sampling_rate = 24000 #  sets the audio playback/sample rate, i.e., how many samples per second the audio contains\n",
    "\n",
    "# 24 kHz is high enough to sound natural and expressive.f\n",
    "# It‚Äôs also more efficient (smaller file sizes) than 44.1kHz used in CD audio.\n",
    "# Bark models are trained on 24kHz, so this is the native sample rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd67a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_prompt: sentence the model to speak.\n",
    "text_prompt = \"\"\"\n",
    "Exactly! [sigh] And the distillation part is where you take a LARGE-model,and compress-it down into a smaller, more efficient model that can run on devices with limited resources.\n",
    "\"\"\"\n",
    "inputs = processor(text_prompt, voice_preset=voice_preset).to(device)\n",
    "\n",
    "# actual audio generation happens\n",
    "speech_output = model.generate(**inputs, temperature = 0.9, semantic_temperature = 0.8)\n",
    "Audio(speech_output[0].cpu().numpy(), rate=sampling_rate) # Play the Generated Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2922fe14",
   "metadata": {},
   "source": [
    "### Bringing it together: Making the Podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../data/podcast_ready_data.pkl', 'rb') as file:\n",
    "    PODCAST_TEXT = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a45bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bark_processor = AutoProcessor.from_pretrained(\"suno/bark\")\n",
    "bark_model = BarkModel.from_pretrained(\"suno/bark\", torch_dtype=torch.float16).to(\"cuda:3\")\n",
    "bark_sampling_rate = 24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6726e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parler_model = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-mini-v1\").to(\"cuda:3\")\n",
    "parler_tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5404be",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker1_description = \"\"\"\n",
    "Laura's voice is expressive and dramatic in delivery, speaking at a moderately fast pace with a very close recording that almost has no background noise.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dfaa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_segments = []\n",
    "sampling_rates = []  # We'll need to keep track of sampling rates for each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b811d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dfc8dd",
   "metadata": {},
   "source": [
    "### Generate the Audio of Speaker_1 `Parler Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af245e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_speaker1_audio(text):\n",
    "    \"\"\"Generate audio using ParlerTTS for Speaker 1\"\"\"\n",
    "    input_ids = parler_tokenizer(speaker1_description, return_tensors=\"pt\").input_ids.to(device)\n",
    "    prompt_input_ids = parler_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    generation = parler_model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\n",
    "    audio_arr = generation.cpu().numpy().squeeze()\n",
    "    return audio_arr, parler_model.config.sampling_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34279de",
   "metadata": {},
   "source": [
    "### Generate the Audio of Speaker_2 `Bark Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1728a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_speaker2_audio(text):\n",
    "    \"\"\"Generate audio using Bark for Speaker 2\"\"\"\n",
    "    inputs = bark_processor(text, voice_preset=\"v2/en_speaker_6\").to(device)\n",
    "    speech_output = bark_model.generate(**inputs, temperature=0.9, semantic_temperature=0.8)\n",
    "    audio_arr = speech_output[0].cpu().numpy()\n",
    "    return audio_arr, bark_sampling_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ae9ca",
   "metadata": {},
   "source": [
    "# `numpy_to_audio_segment` Utility Function Explanation\n",
    "\n",
    "`numpy_to_audio_segment` is a handy utility to convert a raw audio signal in a NumPy array into a format that can be easily manipulated or played using the pydub library‚Äôs `AudioSegment` class.\n",
    "\n",
    "- **`audio_arr`**: A NumPy array representing audio waveform samples, usually floating-point values between -1 and 1.\n",
    "\n",
    "- **`sampling_rate`**: The sample rate (in Hz), e.g., 24000 or 16000, that specifies how many samples per second the audio has.\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Breakdown\n",
    "\n",
    "1. **Convert to 16-bit PCM format**  \n",
    "   - Audio samples usually range between -1 and 1.  \n",
    "   - Multiplying by 32767 scales samples to the 16-bit integer range.  \n",
    "   - `.astype(np.int16)` converts the samples to 16-bit signed integers, which is the standard format for WAV files.\n",
    "\n",
    "2. **Create an in-memory WAV file**  \n",
    "   - `io.BytesIO()` creates an in-memory binary stream (no disk I/O).  \n",
    "   - `wavfile.write()` writes the audio data to this stream as a WAV file with the specified sampling rate.  \n",
    "   - `byte_io.seek(0)` resets the stream position to the beginning for reading.\n",
    "\n",
    "3. **Load as `AudioSegment`**  \n",
    "   - `AudioSegment.from_wav(byte_io)` reads the in-memory WAV data and returns an `AudioSegment` object.  \n",
    "   - This object can be used for audio manipulation, playback, and exporting to other formats.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use This?\n",
    "\n",
    "- Many TTS or audio generation models output raw NumPy arrays.  \n",
    "- `pydub.AudioSegment` provides powerful audio manipulation and export features.  \n",
    "- This function bridges the raw model output to a flexible and easy-to-use audio format.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd8fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_audio_segment(audio_arr, sampling_rate):\n",
    "    \"\"\"Convert numpy array to AudioSegment\"\"\"\n",
    "    # Convert to 16-bit PCM\n",
    "    audio_int16 = (audio_arr * 32767).astype(np.int16)\n",
    "    \n",
    "    # Create WAV file in memory\n",
    "    byte_io = io.BytesIO()\n",
    "    wavfile.write(byte_io, sampling_rate, audio_int16)\n",
    "    byte_io.seek(0)\n",
    "    \n",
    "    # Convert to AudioSegment\n",
    "    return AudioSegment.from_wav(byte_io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab6c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PODCAST_TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e29a4",
   "metadata": {},
   "source": [
    "# What is `import ast` and `ast.literal_eval(PODCAST_TEXT)`?\n",
    "\n",
    "- **`import ast`**:  \n",
    "  Imports Python's Abstract Syntax Trees (AST) module, which helps parse and analyze Python code programmatically.\n",
    "\n",
    "- **`ast.literal_eval()`**:  \n",
    "  A safe way to evaluate a string containing a Python literal or container (like strings, numbers, tuples, lists, dicts, booleans, and `None`) into its corresponding Python object.  \n",
    "  Unlike `eval()`, it **only evaluates literals** and does **not execute arbitrary code**, so it's much safer.\n",
    "\n",
    "---\n",
    "\n",
    "## In this context:\n",
    "\n",
    "- `PODCAST_TEXT` is likely a string that looks like a Python list or dictionary, for example:  \n",
    "  `\"[('Speaker 1', 'Hello!'), ('Speaker 2', 'Hi there!')]\"`\n",
    "\n",
    "- Running `ast.literal_eval(PODCAST_TEXT)` converts this string into an actual Python list of tuples (or whatever structure is represented), so we can work with it as a real Python object.\n",
    "\n",
    "---\n",
    "\n",
    "## Why use it?\n",
    "\n",
    "- When we load data from files (like `.pkl`, `.txt`, or `.json` sometimes) that store Python data structures as strings, we need to convert those strings back to Python objects.\n",
    "- `literal_eval` lets we do this safely and easily.\n",
    "\n",
    "---\n",
    "\n",
    "## Example:\n",
    "\n",
    "```python\n",
    "import ast\n",
    "\n",
    "s = \"[('Speaker 1', 'Hello!'), ('Speaker 2', 'Hi there!')]\"\n",
    "data = ast.literal_eval(s)\n",
    "print(data)\n",
    "# Output: [('Speaker 1', 'Hello!'), ('Speaker 2', 'Hi there!')]\n",
    "print(type(data))\n",
    "# Output: <class 'list'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68647bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "ast.literal_eval(PODCAST_TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c006e6",
   "metadata": {},
   "source": [
    "### Generating the Final Podcast\n",
    "\n",
    "This code implements a Text-to-Speech (TTS) pipeline that:\n",
    "\n",
    "- Takes a podcast script divided into multiple segments, each assigned to a different speaker.\n",
    "- Uses two separate TTS models to generate speech audio for each speaker‚Äôs text:\n",
    "  - One model for Speaker 1‚Äôs voice.\n",
    "  - Another model for Speaker 2‚Äôs voice.\n",
    "- Converts the raw audio outputs (NumPy arrays) into `AudioSegment` objects for easier manipulation.\n",
    "- Concatenates all audio segments in sequence to form a continuous podcast audio track.\n",
    "- Produces a final combined audio file ready for playback or distribution.\n",
    "\n",
    "In short: **it automatically creates a multi-voice podcast episode from written text by synthesizing and merging the voices.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf99803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_audio = None\n",
    "\n",
    "for speaker, text in tqdm(ast.literal_eval(PODCAST_TEXT), desc=\"Generating podcast segments\", unit=\"segment\"):\n",
    "    if speaker == \"Speaker 1\":\n",
    "        audio_arr, rate = generate_speaker1_audio(text)\n",
    "    else:  # Speaker 2\n",
    "        audio_arr, rate = generate_speaker2_audio(text)\n",
    "    \n",
    "    # Convert to AudioSegment (pydub will handle sample rate conversion automatically)\n",
    "    audio_segment = numpy_to_audio_segment(audio_arr, rate)\n",
    "    \n",
    "    # Add to final audio\n",
    "    if final_audio is None:\n",
    "        final_audio = audio_segment\n",
    "    else:\n",
    "        final_audio += audio_segment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e1f138",
   "metadata": {},
   "source": [
    "### Output the Podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df104d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_audio.export(\"../data/outputs/_podcast.mp3\", \n",
    "                  format=\"mp3\", \n",
    "                  bitrate=\"192k\",\n",
    "                  parameters=[\"-q:a\", \"0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a391efb",
   "metadata": {},
   "source": [
    "# Difference Between Raw Audio and AudioSegment\n",
    "\n",
    "## Raw Audio\n",
    "- Basic, low-level audio data represented as a NumPy array.\n",
    "- Contains sound samples (numbers) typically ranging from -1 to 1.\n",
    "- Just the waveform data without metadata or easy playback support.\n",
    "- Output format from many TTS or audio generation models.\n",
    "- Needs extra processing to play, edit, or save as audio files.\n",
    "\n",
    "## AudioSegment (from pydub)\n",
    "- A high-level Python object that wraps raw audio data.\n",
    "- Includes metadata like sample rate, channels, duration.\n",
    "- Supports easy playback, editing (cut, join, fade), and exporting (MP3, WAV).\n",
    "- Automatically handles audio format and sample rate conversions.\n",
    "- Makes audio manipulation and saving straightforward and user-friendly.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "| Aspect        | Raw Audio (NumPy Array)         | AudioSegment (pydub Object)         |\n",
    "|---------------|--------------------------------|------------------------------------|\n",
    "| Data Type    | Array of audio samples (numbers) | Object with audio data + metadata  |\n",
    "| Usage       | Low-level waveform data           | Easy editing, playback, exporting  |\n",
    "| Playback    | Requires additional tools         | Can play and export directly       |\n",
    "| Editing     | Difficult, manual processing      | Built-in functions for edits       |\n",
    "\n",
    "**AudioSegment is used to convert raw audio arrays into a convenient format for working with sound in Python.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4886df9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
