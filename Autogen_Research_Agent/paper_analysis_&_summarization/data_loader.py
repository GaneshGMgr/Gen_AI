import requests
import xml.etree.ElementTree as ET
from scholarly import scholarly

class DataLoader:
    def __init__(self, search_agent=None):
        print("DataLoader Init")
        self.search_agent = search_agent  # Allow search agent to be passed, if needed

    def fetch_arxiv_papers(self, query):
        """
            Fetches top 5 research papers from ArXiv based on the user query.
            If <5 papers are found, expands the search using related topics.
            
            Returns:
                list: A list of dictionaries containing paper details (title, summary, link).
        """
        
        def search_arxiv(query):  # querying the arXiv API to fetch research papers based on a given query
            """Helper function to query ArXiv API."""
            url = f"http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results=5"
            response = requests.get(url)
            if response.status_code == 200:
                root = ET.fromstring(response.text)  # Converts XML string into an ElementTree object
                return [
                    {  # {http://www.w3.org/2005/Atom} is a namespace required for parsing XML correctly
                        "title": entry.find("{http://www.w3.org/2005/Atom}title").text,
                        "summary": entry.find("{http://www.w3.org/2005/Atom}summary").text,
                        "link": entry.find("{http://www.w3.org/2005/Atom}id").text
                    }
                    for entry in root.findall("{http://www.w3.org/2005/Atom}entry")
                ]
            return []

        papers = search_arxiv(query)

        if len(papers) < 5 and self.search_agent:  # If fewer than 5 papers and self.search_agent exists, expand search
            # self.search_agent.generate_reply() is likely an LLM-based agent (e.g., GPT) that generates 3 related research topics for 
            # the given query.
            related_topics_response = self.search_agent.generate_reply(  # Ask the search agent for related research topics
                messages=[{"role": "user", "content": f"Suggest 3 related research topics for '{query}'"}]
            )
            related_topics = related_topics_response.get("content", "").split("\n")  # Extracts response and splits by newline into list

            for topic in related_topics:
                topic = topic.strip()  # Remove extra spaces
                if topic and len(papers) < 5:
                    new_papers = search_arxiv(topic)  # Again search_arxiv() for this 3 related research topics generated by LLM-based.
                    papers.extend(new_papers)  # Add new papers to the list
                    papers = papers[:5]  # Ensure max 5 papers

        return papers

    def fetch_google_scholar_papers(self, query):
        """
        Fetches top 5 research papers from Google Scholar.
        If fewer than 5 papers are found, expands the search using related topics.
        Returns:
            list: A list of dictionaries containing paper details (title, summary, link).
        """
        papers = []
        search_results = scholarly.search_pubs(query) #  search for research papers on Google Schola

        # Get the first 5 papers from Google Scholar
        for i, paper in enumerate(search_results):
            if i >= 5:
                break
            papers.append({
                "title": paper["bib"]["title"],
                "summary": paper["bib"].get("abstract", "No summary available"),
                "link": paper.get("pub_url", "No link available")
            })
        
        # If fewer than 5 papers, expand search using related topics
        if len(papers) < 5 and self.search_agent:  # Assuming self.search_agent is defined
            # Use the search agent to suggest related research topics
            related_topics_response = self.search_agent.generate_reply(
                messages=[{"role": "user", "content": f"Suggest 3 related research topics for '{query}'"}]
            )
            related_topics = related_topics_response.get("content", "").split("\n")

            for topic in related_topics:
                topic = topic.strip()  # Clean up the topic text
                if topic and len(papers) < 5:
                    # Re-query Google Scholar using the new related topic
                    new_papers = scholarly.search_pubs(topic)
                    for i, paper in enumerate(new_papers):
                        if len(papers) >= 5:
                            break
                        papers.append({
                            "title": paper["bib"]["title"],
                            "summary": paper["bib"].get("abstract", "No summary available"),
                            "link": paper.get("pub_url", "No link available")
                        })

        return papers
